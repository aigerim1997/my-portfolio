# My Projects

<br>
### <span style="color:black"> Identifying Star Trek Fans among Star Wars Fans: Accuracy-Interpretability Trade-off in Supervised Machine learning </span> <a href="https://github.com/aigerim1997/my-portfolio/tree/master/accuracy-interpretability-tradeoff" target="_blank"> (View Code) </a>
<p align="justify">
The goal of this project was to identify the most important factors in predicting whether a person who likes Star Wars is also a fan of the Star Trek franchise. As part of this project, a trade-off between interpretability and predictive power has been examined by comparing two methods: logistic regression with lasso regularization, which is pretty straightforward to interpret, and random forest, which gives better prediction results but is more 'black-box' and thus requires model agnostic methods to make the model interpretable.
</p>
*Tags: Classification, Logistic Regression (Lasso), Random Forest, Permutation Based Variable Importance, Partial Dependence Plots, R*

<br>
### <span style="color:black"> Predicting Buying Intention of E-commerce Customers Using a Feedforward Neural Network </span> <a href="https://github.com/aigerim1997/my-portfolio/tree/master/fnn" target="_blank"> (View Code) </a>
<p align="justify">
In this project, an artificial neural network (ANN) was used to classify online shoppers according to their
intentions to make a purchase, the research objective being: “To predict whether an e-commerce customer
will finalize their purchase using clickstream data, as accurately as possible.” This is an individual project which was aimed at introducing myself to deep learning, learn how to train, tune and test a feedforward neural network, and to familiarize myself with the Keras package. 
</p>
*Tags: Classification, Feedforward Neural Network, Keras*

<br>
### <span style="color:black"> Segmentation of Coffee Drinkers on Campus Using Latent Class Analysis </span> <a href="https://github.com/aigerim1997/my-portfolio/tree/master/clustering-lca" target="_blank"> (View Code) </a>
<p align="justify">
This project was done in a team of 4, where we conducted a survey and used Latent Class Analysis to identify segments of coffee drinkers at Erasmus University campus. In this research, we assessed what groups and which characteristics can be distinguished based on attributes that students at the Erasmus University find important when buying coffee. LCA was preferred over other clustering techniques since it is suitable for working with categorical variables, which were prevalent in the survey data (available on GitHub). A model with two clusters was chosen since it outperformed others in terms of statistical fit (lowest BIC value) and business utility (balanced clusters, more interpretable). 
<br>
The first cluster was identified as ‘Foodies’, since they value the ability to buy additional food and quality over other attributes. This group consisted mostly of Bachelor students who turned out to be less frequent coffee buyers. The second segment was identified as ‘Lazy’, as proximity to study spots was the most important consideration for this group. These students are usually Master students who only drink coffee for the caffeine or out of habit. In conclusion, a recommendation for the fictional coffee provider on campus was that in order to attract more “Foodies”, the provider can increase the quality of coffee or offer combo deals that promote, for example, discounts when buying both food and coffee. As for the “Lazy” segment, an additional coffee machine could be placed in close proximity to popular study places.
</p>
*Tags: Clustering, Latent Class Analysis, R, Segmentation*

<br>
### <span style="color:black"> Graph-Based Text Summarization of Earnings Calls Transcripts: The Effects of Changing Sentence-Vector Representation </span> <a href="https://github.com/aigerim1997/my-portfolio/tree/master/text-summarization" target="_blank"> (View Code) </a>
<p align="justify">
This was my Master graduation project which illustrated the use of an NLP tool called automatic summarization algorithm in the domain of corporate disclosure. The goal of this study was to compare a graph-based summarization algorithm that incorporates semantic inter-sentence similarities to one that only takes into account lexical similarities, by producing summaries of 20 earnings call transcripts. The two algorithms differ in their sentence-vector representations: the lexical summarization algorithm employs a bag-of-words (BOW) model to obtain sentence-vectors, while the semantic summarization algorithm makes use of sentence embeddings (Doc2Vec). 
<br>
After producing the summaries and evaluating them with a help of six human judges, it has been concluded that the semantic algorithm produces summaries containing information that is more useful for making investment decisions about a company. In addition, a qualitative comparison of the summaries produced using the two approaches has revealed that the ones produced by the semantic summarization algorithm contain potentially more useful sentences. This was evident from the semantic algorithm including relatively fewer redundant transition sentences, and more information related to a firm’s performance. Moreover, the semantic summaries proved to be less repetitive and therefore provide more diverse information, which is beneficial in the case of corporate disclosure, since the users would prefer to learn about different aspects of a firm’s performance in order to make an investment decision.
</p>
*Tags: NLP, Automatic Text Summarization, TextRank, Doc2Vec, Bag-of-Words, R, Python*

