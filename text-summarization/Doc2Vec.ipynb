{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the continuation of Section 4.1 of the Text Summarization_Main.R script.\n",
    "\n",
    "4.1.1 Obtaining Doc2Vec embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import unihandecode\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer= PorterStemmer()\n",
    "import itertools as it\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>We are delighted that you are able to be with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The presentation will, as usual, follow our cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Firstly, we will begin with an overview of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>We'd also like to point out that we are only t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>So please ask your question only through our w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>334</td>\n",
       "      <td>The rest of these items are all consistent wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>335</td>\n",
       "      <td>DD&amp;A, we did lower that guidance range, really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>336</td>\n",
       "      <td>And then interest expense is down, reflecting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>337</td>\n",
       "      <td>And you'll see the next step down on interest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>338</td>\n",
       "      <td>So with that, Yolanda, I think we'll open up t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                               text\n",
       "0        2  We are delighted that you are able to be with ...\n",
       "1        3  The presentation will, as usual, follow our cu...\n",
       "2        4  Firstly, we will begin with an overview of the...\n",
       "3        5  We'd also like to point out that we are only t...\n",
       "4        6  So please ask your question only through our w...\n",
       "..     ...                                                ...\n",
       "333    334  The rest of these items are all consistent wit...\n",
       "334    335  DD&A, we did lower that guidance range, really...\n",
       "335    336  And then interest expense is down, reflecting ...\n",
       "336    337  And you'll see the next step down on interest ...\n",
       "337    338  So with that, Yolanda, I think we'll open up t...\n",
       "\n",
       "[3568 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a pattern for the file names to load:\n",
    "file_name = r'C:\\Users\\Aigerim\\Documents\\R\\Thesis\\transcripts_df\\transcript{}.csv'\n",
    "\n",
    "# Create an empty dataframe to store all the sentences:\n",
    "columns = ['doc_id','text'] # Columns in the data frame\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df = df.fillna(0) \n",
    "\n",
    "# Import the data frames with sentences into a single data frame:\n",
    "for k in range(1,21):    \n",
    "    # Read a data frame corresponding to a transcript (features: sentence ids and sentences):\n",
    "    df1 = pd.read_csv(file_name.format(k), encoding='latin-1')\n",
    "    df1 = df1.drop(columns=[\"Unnamed: 0\"])\n",
    "    df = df.append(df1)\n",
    "df.to_csv(r'~/Documents/R/Thesis/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and pre-process the sentences:\n",
    "sentences_df = df\n",
    "nrow = sentences_df.shape[0] # Extract the total number of sentences\n",
    "for r in range(0, nrow):\n",
    "    sentences_df.iloc[r,1] = unihandecode.unidecode(sentences_df.iloc[r,1]) # convert to ASCII \n",
    "    sentences_df.iloc[r,1] = re.sub(r'[^\\w\\s\\\"!\\?]',' ', sentences_df.iloc[r,1]) # Remove punctuation except ! ? \"\n",
    "    sentences_df.iloc[r,1] = re.sub(r\"\\d+\", \" 123\", sentences_df.iloc[r,1]) # Replace numbers with '123'\n",
    "    sentences_df.iloc[r,1] = sentences_df.iloc[r,1].strip() # Remove redundant white space\n",
    "    word_tokens = word_tokenize(sentences_df.iloc[r,1]) # Tokenize each sentence on word level\n",
    "    stemmed = [stemmer.stem(word) for word in word_tokens] # Stemming using Porter's algorithm\n",
    "    sentences_df.iloc[r,1] = ' '.join(stemmed) # Paste the tokens into one string (for each sentence)\n",
    "    \n",
    "# Create a Tagged document with each sentence tagged (an object needed as an input to the Doc2Vec model):\n",
    "tagged_data = [TaggedDocument(words=nltk.word_tokenize(_d.lower()), \n",
    "                                  tags=[str(i)]) for i, _d in enumerate(sentences_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 2),\n",
       " (25, 3),\n",
       " (25, 4),\n",
       " (50, 2),\n",
       " (50, 3),\n",
       " (50, 4),\n",
       " (100, 2),\n",
       " (100, 3),\n",
       " (100, 4)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters tuning: setting values for embedding vector size and context window size:\n",
    "vec_size = [25, 50, 100] # Embedding vector size variations used for tuning\n",
    "window_size = [2,3,4] # Window size variations used for tuning\n",
    "\n",
    "# Expand the grid:\n",
    "a = [i for i in it.product(vec_size,window_size)] # 9 combinations\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.9588004484304933\n",
      "2\n",
      "0.9613228699551569\n",
      "3\n",
      "0.9613228699551569\n",
      "4\n",
      "0.9624439461883408\n",
      "5\n",
      "0.9641255605381166\n",
      "6\n",
      "0.9630044843049327\n",
      "7\n",
      "0.9641255605381166\n",
      "8\n",
      "0.9638452914798207\n",
      "9\n",
      "0.9655269058295964\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning: the loop that iterates over all hyperparameter configurations (9 in total)\n",
    "for i in range (0, 9):\n",
    "    print(i+1)\n",
    "    \n",
    "    # Model specification:\n",
    "    model = Doc2Vec(vector_size= a[i][0],\n",
    "                alpha=0.01, # Learning rate at the start of the training\n",
    "                min_alpha=0.0001, # Learning rate by the end of the training (last epoch)\n",
    "                min_count=2, # Ignores words with frequencies lower than this\n",
    "                window_size= a[i][1],\n",
    "                dm =1, # Distributed memory model\n",
    "                max_vocab_size=None,\n",
    "                epochs = 100, # Train for 100 epochs\n",
    "                seed=123) # Set random seed \n",
    "\n",
    "    # Build a vocabulary:\n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    # Train the model:\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    \n",
    "    # Save the model:\n",
    "    model.save(\"d2v.model\")\n",
    "    \n",
    "    # Load the model: \n",
    "    model= Doc2Vec.load(\"d2v.model\")\n",
    "    \n",
    "    # Store obtained document embeddings as a data frame:\n",
    "    embeddings = pd.DataFrame(model.docvecs.vectors_docs)\n",
    "    \n",
    "    # Count how many times the model ranks the sentence as top 1 similar to itself and calculate the percentage out of the total number of sentences in the corpus:\n",
    "    ranks = []\n",
    "    second_ranks = []\n",
    "    \n",
    "    for doc_id in range(len(tagged_data)):\n",
    "        inferred_vector = model.infer_vector(tagged_data[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "        rank = [docid for docid, sim in sims].index(str(doc_id))\n",
    "        ranks.append(rank)\n",
    "        \n",
    "        second_ranks.append(sims[1])\n",
    "\n",
    "    counter = collections.Counter(ranks)\n",
    "    print(counter[0]/len(tagged_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal model:\n",
    "# Model specification:\n",
    "model = Doc2Vec(vector_size=a[6][0],\n",
    "                alpha=0.01, \n",
    "                min_alpha=0.0001,\n",
    "                min_count=2, # Ignores words with frequencies lower than this\n",
    "                window_size= a[6][1],\n",
    "                dm =1,\n",
    "                max_vocab_size=None,\n",
    "                epochs = 100, seed=123)\n",
    "    \n",
    "# Build a vocabulary:\n",
    "model.build_vocab(tagged_data)\n",
    "   \n",
    "# Train the model:\n",
    "model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "model.save(\"d2v.model\")\n",
    "    \n",
    "# Load the model: \n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "    \n",
    "# Display obtained document embeddings:\n",
    "embeddings = pd.DataFrame(model.docvecs.vectors_docs)\n",
    "    \n",
    "# Save the data frame with all sentence embeddings into a csv file: to be used in the rest of the Text Summarization_Main.R script.\n",
    "filename = r'~/Documents/R/Thesis/embeddings.csv'\n",
    "embeddings.to_csv(filename, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
